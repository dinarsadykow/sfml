{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train  (200000, 4)\n",
      "df_test  (170179, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinar/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df  (370179, 5)\n",
      "df_dict  370179\n",
      "CPU times: user 10.3 s, sys: 2.85 s, total: 13.2 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup # Превращалка html в текст.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df_sample = pd.read_csv('sampleSubmission.csv', sep = '\\t')\n",
    "df_train = pd.read_csv('train.csv', sep = '\\t')\n",
    "df_test = pd.read_csv('test.csv', sep = '\\t')\n",
    "print('df_train ',df_train.shape)\n",
    "print('df_test ',df_test.shape)\n",
    "\n",
    "df_train.loc[:, 'sample'] = 'train'\n",
    "df_test.loc[:, 'sample'] = 'test'\n",
    "df = df_test.append(df_train).reset_index(drop=True)\n",
    "print('df ',df.shape)\n",
    "\n",
    "df_dict = df[['description']].to_dict()['description']\n",
    "df_name = df[['name']].to_dict()['name']\n",
    "print('df_dict ',len(df_dict))\n",
    "\n",
    "del df_dict\n",
    "del df\n",
    "del df_sample\n",
    "del df_train\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name  370179\n"
     ]
    }
   ],
   "source": [
    "print('df_name ',len(df_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посчитаем лишние слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 1.83 s, total: 31.1 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = {}\n",
    "for line in list(df_dict_re.keys()):\n",
    "    for word in df_dict_re[line].split():\n",
    "        if words.get(word):\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "print(len(words))\n",
    "\n",
    "df_words = pd.DataFrame(list(words.items())).sort_values(by=1, ascending=False).reset_index(drop=True)\n",
    "df_words['len_0'] = df_words[0].map(lambda x: len(x) )\n",
    "df_block_words = df_words[df_words['len_0'] <4] # лишние слова - когда длина слов меньше или равно 3-х"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 1: подсчет слов вручную в словарь и перевод в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# pstrong = re.compile('<p><strong>(.+?)</strong></p>', re.S)\n",
    "# p = re.compile('<p>.+?</p>', re.S)\n",
    "# ulli = re.compile(\"<ul>\\ <li>(.+?)</li>\\ </ul>\", re.S)\n",
    "# ''.join(ulli.split(''.join(p.split(''.join(pstrong.split(df_dict[line]))))))\n",
    "\n",
    "df_dict_re = {}\n",
    "for line in df_dict.keys():\n",
    "    df_dict_re[line] = ' '.join(re.findall(\"[А-ЯЁа-яё]+[\\:|\\,|\\.]*\", df_dict[line]))\n",
    "print('df_dict_re ',len(df_dict_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 s, sys: 2.17 s, total: 27.7 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_to_number_for_each_texts = {}\n",
    "for text in df_dict_re.values():\n",
    "    word_to_number_for_each_texts[text] = {}\n",
    "    for word in text.split():\n",
    "        if word_to_number_for_each_texts[text].get(word):\n",
    "            word_to_number_for_each_texts[text][word] += 1\n",
    "        else:\n",
    "            word_to_number_for_each_texts[text][word] = 1\n",
    "len(word_to_number_for_each_texts)\n",
    "\n",
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 26.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# df_dict_re_matrix = pd.DataFrame(list(word_to_number_for_each_texts.values())).fillna(0)\n",
    "# df_dict_re_matrix.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 851 ms, sys: 1.96 s, total: 2.81 s\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import scipy.sparse as sp\n",
    "from nltk.text import TextCollection\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "temp_matrix = count_vect.fit_transform(df_dict_re.values()) # temp_matrix эта промежуточная матрица, понадобится в следующем кейсе,\n",
    "                                              # для вычисления tf-idf в данной матрице хранятся \n",
    "matrix_counts = temp_matrix.toarray()\n",
    "\n",
    "words = [x[0] for x in sorted(count_vect.vocabulary_.items(), key=lambda x: x[1])] # список слов, \n",
    "                                                                                   # чтобы сделать красивую шапку\n",
    "print(len(words))\n",
    "# pd.DataFrame(matrix_counts, columns=words)  # при создании DataFrame передадим подготовленный список слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 2: через вектора слов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для новостной заметки можно составить ее словарь, а также посчитать частоты всех слов. В итоге мы получим представление текста в виде вектора. В этом векторе координаты будут называться по соответствующим словам, а смещение по данной координате будет показывать частота. <br>\n",
    "При составлении словаря будем учитывать только значимые слова - существительные, прилагательные и глаголы. Помимо этого предусмотрим возможность учитывать часть речи слова, прибавляя ее у начальной форме.<br>\n",
    "Для разделения текста на слова используем простейший алгоритм: слово - это последовательность букв русского алфавита среди которых может попадаться дефис. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Морфологический анализатор.\n",
    "from collections import Counter # Не считать же частоты самим.\n",
    "import math # Корень квадратный.\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "\n",
    "posConv={'ADJF':'_ADJ','NOUN':'_NOUN','VERB':'_VERB'}\n",
    "meaningfullPoSes=['ADJF', 'NOUN', 'VERB']\n",
    "def getArticleDictionary(text, needPos=None):\n",
    "    words=[a[0] for a in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "    reswords=[]\n",
    "    for w in words:\n",
    "        wordform=morph.parse(w)[0]\n",
    "        if wordform.tag.POS in meaningfullPoSes:\n",
    "            if needPos!=None:\n",
    "                reswords.append(wordform.normal_form+posConv[wordform.tag.POS])\n",
    "            else:\n",
    "                reswords.append(wordform.normal_form)\n",
    "    return Counter(reswords)\n",
    "\n",
    "# print(dict(getArticleDictionary(df_dict_re[1], True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dict_vector  370179\n",
      "CPU times: user 3h 54min 23s, sys: 33.2 s, total: 3h 54min 56s\n",
      "Wall time: 3h 56min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_dict_vector = {}\n",
    "for line in df_dict.keys():\n",
    "    df_dict_vector[line] = (dict(getArticleDictionary(df_dict[line], True)))\n",
    "print('df_dict_vector ',len(df_dict_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 23 µs, total: 37 µs\n",
      "Wall time: 43.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import yaml\n",
    "# with open('df_dict.yml', 'w') as f:\n",
    "#     yaml.dump(df_dict_vector, f, default_flow_style=False)\n",
    "    \n",
    "# with open('df_dict.yml', 'r') as f:\n",
    "#     df_dict_vector = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_name_vector  370179\n",
      "df_name to dict complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "df_name_vector = {}\n",
    "for line in df_name.keys():\n",
    "    df_name_vector[line] = (dict(getArticleDictionary(df_name[line], True)))\n",
    "print('df_name_vector ',len(df_name_vector))\n",
    "\n",
    "with open('df_name.yml', 'w') as f:\n",
    "    yaml.dump(df_name_vector, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_dict_vector_matrix = pd.DataFrame(list(df_dict_vector.values())).fillna(0)\n",
    "# df_dict_vector_matrix.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 4. TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter=CountVectorizer()\n",
    "# Просим посчитать частоты слов.\n",
    "res=counter.fit_transform([df_dict[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['обязанности', 'работа', 'с', 'клиентом', 'в', 'салоне', 'выезд', 'на', 'замер', 'создание', 'дизайн-проекта', 'расчеты', 'ведение', 'документации', 'заключение', 'договоров', 'требования', 'опыт', 'работы', 'желателен', 'уверенный', 'пользователь', 'пк', 'грамотная', 'речь', 'желание', 'учиться', 'и', 'развиваться', 'условия', 'трудоустройство', 'по', 'тк', 'обучение', 'стажировка', 'оклад', 'от', 'личных', 'продаж', 'удобный', 'график', 'обязанности работа', 'работа с', 'с клиентом', 'клиентом в', 'в салоне', 'салоне выезд', 'выезд на', 'на замер', 'замер создание', 'создание дизайн-проекта', 'дизайн-проекта расчеты', 'расчеты ведение', 'ведение документации', 'документации заключение', 'заключение договоров', 'договоров требования', 'требования опыт', 'опыт работы', 'работы желателен', 'желателен уверенный', 'уверенный пользователь', 'пользователь пк', 'пк грамотная', 'грамотная речь', 'речь желание', 'желание учиться', 'учиться и', 'и развиваться', 'развиваться условия', 'условия трудоустройство', 'трудоустройство по', 'по тк', 'тк обучение', 'обучение стажировка', 'стажировка оклад', 'оклад от', 'от личных', 'личных продаж', 'продаж удобный', 'удобный график']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# При помощи ngram_range=(1,2) говорим, что хотим извлекать слова и пары слов.\n",
    "# token_pattern показывает регулярное выражение, которому должны соответствовать слова.\n",
    "counter12=CountVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "# Проводим анализ, получаем список найденных n-грамм.\n",
    "analyze = counter12.build_analyzer()\n",
    "print(analyze(df_dict[0]))\n",
    "# Считаем частоты, видим, что слова не приводились к начальной форме.\n",
    "res=counter12.fit_transform([df_dict[0]])\n",
    "print(counter12.vocabulary_.get('уверенный'))\n",
    "# print(counter12.vocabulary_.get('мама'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'работа': 2}\n",
      "---\n",
      "{'работа': 2}\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2 # Морфологический анализатор.\n",
    "from collections import Counter # Не считать же частоты самим.\n",
    "import math # Корень квадратный.\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "\n",
    "def getMeaningfullWords(text):\n",
    "    words=[]\n",
    "    tokens=re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv=morph.parse(t)\n",
    "        for p in pv:\n",
    "            if p.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                words.append(p.normal_form)\n",
    "                break\n",
    "    return words\n",
    "\n",
    "c=[' '.join(getMeaningfullWords(df_dict[0]))]\n",
    "c2=[' '.join([morph.parse(r)[0].normal_form for r in re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', df_dict[0])])]\n",
    "\n",
    "lemmaCounter=CountVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1=analyze(c[0])\n",
    "res2=lemmaCounter.fit_transform(c)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n",
    "print(\"---\")\n",
    "res1=analyze(c2[0])\n",
    "res2=lemmaCounter.fit_transform(c2)\n",
    "print({w:res2[0][0,lemmaCounter.vocabulary_[w]] for w in lemmaCounter.vocabulary_ if res2[0][0,lemmaCounter.vocabulary_[w]]>1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><strong>Обязанности:</strong></p> <ul> <li>Работа с клиентом в салоне,выезд на замер ,создание дизайн-проекта,расчеты,ведение документации,заключение договоров</li> </ul> <p> </p> <p><strong>Требования:</strong></p> <ul> <li>Опыт работы желателен,уверенный пользователь ПК,грамотная речь,желание учиться и развиваться</li> </ul> <p> </p> <p><strong>Условия:</strong></p> <ul> <li>Трудоустройство по ТК, обучение+стажировка,оклад+% от личных продаж,удобный график</li> </ul>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "tfCounter=TfidfVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = tfCounter.build_analyzer()\n",
    "res=tfCounter.fit_transform([df_dict[0]])\n",
    "print(df_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_article=6\n",
    "res2=analyze(df_dict[1])\n",
    "tfs=list(set(res[0, tfCounter.vocabulary_.get(k)] for k in res2 if k in tfCounter.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print({w:res[0, tfCounter.vocabulary_[w]] for w in res2 if w in tfCounter.vocabulary_.keys() and res[id_article][0, tfCounter.vocabulary_[w]] in tfs2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
