{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/skillfactorylogo.png\"></center>\n",
    "\n",
    "<h1><center>Курс \"Практический Machine Learning\"</center></h1>\n",
    "<h3><center>Шестаков Андрей</center></h3>\n",
    "<hr>\n",
    "<h2><center>Задача регрессии</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (18,12)\n",
    "\n",
    "from ipywidgets import interact, IntSlider, FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Задача-регрессии\" data-toc-modified-id=\"Задача-регрессии-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Задача регрессии</a></div><div class=\"lev2 toc-item\"><a href=\"#Меры-качества\" data-toc-modified-id=\"Меры-качества-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Меры качества</a></div><div class=\"lev3 toc-item\"><a href=\"#Стандартные-меры\" data-toc-modified-id=\"Стандартные-меры-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Стандартные меры</a></div><div class=\"lev3 toc-item\"><a href=\"#Относительные-меры\" data-toc-modified-id=\"Относительные-меры-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Относительные меры</a></div><div class=\"lev3 toc-item\"><a href=\"#Несимметричные-меры\" data-toc-modified-id=\"Несимметричные-меры-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Несимметричные меры</a></div><div class=\"lev3 toc-item\"><a href=\"#Пороговые-меры\" data-toc-modified-id=\"Пороговые-меры-214\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Пороговые меры</a></div><div class=\"lev1 toc-item\"><a href=\"#KNN-и-деревья-в-регрессии\" data-toc-modified-id=\"KNN-и-деревья-в-регрессии-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>KNN и деревья в регрессии</a></div><div class=\"lev3 toc-item\"><a href=\"#Метод-ближайшего-соседа\" data-toc-modified-id=\"Метод-ближайшего-соседа-301\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Метод ближайшего соседа</a></div><div class=\"lev3 toc-item\"><a href=\"#Деревья-решений-для-регрессии\" data-toc-modified-id=\"Деревья-решений-для-регрессии-302\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Деревья решений для регрессии</a></div><div class=\"lev2 toc-item\"><a href=\"#Исследование-остатков\" data-toc-modified-id=\"Исследование-остатков-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Исследование остатков</a></div><div class=\"lev2 toc-item\"><a href=\"#Линейная-регрессия\" data-toc-modified-id=\"Линейная-регрессия-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Линейная регрессия</a></div><div class=\"lev3 toc-item\"><a href=\"#Градиентный-спуск\" data-toc-modified-id=\"Градиентный-спуск-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Градиентный спуск</a></div><div class=\"lev2 toc-item\"><a href=\"#Природа-зависимости\" data-toc-modified-id=\"Природа-зависимости-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Природа зависимости</a></div><div class=\"lev4 toc-item\"><a href=\"#Пример\" data-toc-modified-id=\"Пример-3301\"><span class=\"toc-item-num\">3.3.0.1&nbsp;&nbsp;</span>Пример</a></div><div class=\"lev2 toc-item\"><a href=\"#Ладно,-давайте-дальше-в-sklearn-=)\" data-toc-modified-id=\"Ладно,-давайте-дальше-в-sklearn-=)-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Ладно, давайте дальше в sklearn =)</a></div><div class=\"lev3 toc-item\"><a href=\"#Переобучениенедообучение,-мультиколлинеарность-и-регуляризация\" data-toc-modified-id=\"Переобучениенедообучение,-мультиколлинеарность-и-регуляризация-341\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Переобучение\\недообучение, мультиколлинеарность и регуляризация</a></div><div class=\"lev2 toc-item\"><a href=\"#Линейная-регрессия-и-выбросы\" data-toc-modified-id=\"Линейная-регрессия-и-выбросы-35\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Линейная регрессия и выбросы</a></div><div class=\"lev3 toc-item\"><a href=\"#Robust-Estimators\" data-toc-modified-id=\"Robust-Estimators-351\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Robust Estimators</a></div><div class=\"lev1 toc-item\"><a href=\"#Добавим-остальные-признаки-в-простую-модель-модель\" data-toc-modified-id=\"Добавим-остальные-признаки-в-простую-модель-модель-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Добавим остальные признаки в простую модель модель</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача регрессии\n",
    "\n",
    "Вспоминаем, что в регрессии восстанавливается вещественная величина - объемы продаж, стоимость дома, количество \"лайков\" и тп.\n",
    "\n",
    "Соответственно, нужно использовать как-то измерять ошибку моделей, относительно истинных значений целевой переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меры качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $y^{(i)}$ - значение целевой переменной для $i$-го объекта, а $\\hat{y}^{(i)}=a(x^{(i)})$ - её оценка алгоритмом $a(x)$.\n",
    "\n",
    "\n",
    "### Стандартные меры\n",
    "\n",
    "Наиболее распространенные меры качества это:\n",
    "\n",
    "** (R)MSE ((Root) Mean Squared Error) - (Корень из) Среднеквадратичное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\sqrt{\\frac{1}{N}\\sum\\limits_i^N (y^{(i)} - \\hat{y}^{(i)})^2}$$\n",
    "\n",
    "* Штрафует большие ошибки сильней, чем маленькие\n",
    "* Корень берут, чтобы ошибка измерялась в тех же шкалах, что и целевая переменная\n",
    "\n",
    "** MAE (Mean Absolute Error) - Среднее абсолютное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\frac{1}{N}\\sum\\limits_i |y^{(i)} - \\hat{y}^{(i)}|$$\n",
    "\n",
    "* Большая устойчивость к выбросам\n",
    "* Интуитивно понятна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src='img/metric-example.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная критика этих мер заключается в том, что\n",
    "\n",
    "* MSE и MAE позволяет сравнивать качество моделей между собой, но дают плохое представление о том насколько хорошо решена задача\n",
    "* Решение с MSE=10\n",
    "    * может быть плохим, если $y\\in [0, 10]$ \n",
    "    * хорошим, если $y\\in [1000, 1000000]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 100\n",
    "y_hat = np.linspace(0, 300, 151)\n",
    "\n",
    "error1 = (y-y_hat)**2\n",
    "error2 = abs(y-y_hat)\n",
    "\n",
    "plt.plot(y_hat, error1, label='MSE')\n",
    "plt.plot(y_hat, error2, label='MAE')\n",
    "plt.xlabel('$\\hat{y}$')\n",
    "plt.ylabel('Error')\n",
    "plt.title('true value y = %.1f' % y)\n",
    "plt.legend()\n",
    "plt.ylim(0, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Относительные меры \n",
    "\n",
    "** RSE (Relative Squared Error) - Относительное квадратичное отклонение**\n",
    "\n",
    "$$ L(a, y) = \\sqrt\\frac{\\sum\\limits_i (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum\\limits_i (y^{(i)} - \\bar{y})^2}$$\n",
    "\n",
    "** RAE (Relative Absolute Error) - Относительное абсолютное отклонение (?)**\n",
    "\n",
    "$$ L(a, y) = \\frac{\\sum\\limits_i |y^{(i)} - \\hat{y}^{(i)}|}{\\sum\\limits_i |y^{(i)} - \\bar{y}|}$$\n",
    "\n",
    "В данном случае, в качестве \"бейзлайна\" выступает среднее предсказание (или какой-то очень простой алгоритм)\n",
    "\n",
    "** MAPE (Mean Absolute Persentage Error) - Среднее абсолютное отклонение в процентах **\n",
    "\n",
    "$$ L(a, y) = \\frac{100}{N} \\sum\\limits_i\\frac{\\left| y^{(i)} - \\hat{y}^{(i)} \\right|}{|y^{(i)}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 100\n",
    "y_mean = 80\n",
    "y_hat = np.linspace(0, 300, 151)\n",
    "\n",
    "error1 = (y-y_hat)**2/(y-y_mean)**2\n",
    "error2 = abs(y-y_hat)/abs(y-y_mean)\n",
    "error3 = abs(y-y_hat)/abs(y)\n",
    "\n",
    "plt.plot(y_hat, error1, label='RSE')\n",
    "plt.plot(y_hat, error2, label='RAE')\n",
    "plt.plot(y_hat, error3, label='MAPE')\n",
    "plt.xlabel('$\\hat{y}$')\n",
    "plt.ylabel('Error')\n",
    "plt.title('true value y = %.1f' % y)\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несимметричные меры\n",
    "\n",
    "** RMSLE (Root Mean Squared Logarithmic Error) - (?!)**\n",
    "\n",
    "$$ L(a, y) = \\sqrt{\\frac{1}{N}\\sum\\limits_i^N(\\log(y^{(i)} + 1) - \\log(\\hat{y}^{(i)} + 1))^2}$$\n",
    "\n",
    "* Почему мера ошибки модели может быть несимметричной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 100\n",
    "y_hat = np.linspace(0, 300, 151)\n",
    "# log error\n",
    "error1 = np.sqrt((np.log(y+1) - np.log(y_hat + 1))**2)\n",
    "\n",
    "# squared error\n",
    "error2 = (y - y_hat)**2 /1000.\n",
    "\n",
    "plt.plot(y_hat, error1, label='RMSLE')\n",
    "plt.plot(y_hat, error2, label='MSE')\n",
    "plt.xlabel('$\\hat{y}$')\n",
    "plt.ylabel('Error')\n",
    "plt.title('true value y = %.1f' % y)\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пороговые меры\n",
    "\n",
    "** Threshold Errors - с точностью до порога**\n",
    "\n",
    "Бывают кейсы, когда нас не интересует точное значение ошибки. Тогда, например, мы можем считать количество долю случаев, когда отклонение модели превышает некоторый порог $\\theta$\n",
    "\n",
    "$$ L(a, y) = \\frac{1}{N}\\sum\\limits_i \\left[ |y^{(i)} - \\hat{y}^{(i)}| < \\theta \\right] $$\n",
    "$$ L(a, y) = \\frac{1}{N}\\sum\\limits_i \\left[ \\frac{ |y^{(i)} - \\hat{y}^{(i)}|}{ |y^{(i)} - \\bar{y}|} < \\theta \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN и деревья в регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод ближайшего соседа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод минимально отличается от варианта с классификацией. <br\\> По прежнему считаем меру \"близости\" между объектами, а затем усредняем значения целевого признака у *k* ближайших соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим данные по характеристикам автомобилей Honda Accord. Названия столбцов говорят сами за себя.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/accord_sedan_training.csv')\n",
    "df_test = pd.read_csv('data/accord_sedan_testing.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(y='price', x='mileage', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотроим регрессор на k ближайших соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values\n",
    "\n",
    "X_test = df_test.mileage.values.reshape(-1, 1)\n",
    "y_test = df_test.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10, \n",
    "                          weights='uniform', \n",
    "                          metric='manhattan')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = knn.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='blue')\n",
    "plt.xlabel('mileage')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_knn(k=5):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights='uniform', metric='manhattan')\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "    y_hat = knn.predict(x)\n",
    "    plt.xlabel('mileage')\n",
    "    plt.ylabel('price')\n",
    "\n",
    "    \n",
    "    plt.scatter(X_train, y_train, c='r', label='actual data')\n",
    "    plt.plot(x, y_hat, c='b', label='knn, $k=%d$' % k)\n",
    "    plt.legend(loc=2)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = interact(plot_knn, k=IntSlider(min=1, max=20, value=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деревья решений для регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним себе на основе чего строится дерево решений. В каждом новом узле выбирается признак и его значение, максимизирущее прирост определенности в классификации это были \n",
    "* miscalssification error\n",
    "* entropy\n",
    "* gini\n",
    "\n",
    "В случае регрессии в качестве меры неопределенности можно рассматривать квадрат отклонения значений целевой переменной относительно среднего в узле\n",
    "$$I(S) = \\frac{1}{|S|} \\sum\\limits_{i \\in S} (y_i - c)^2 $$ \n",
    "$$ c = \\frac{1}{|S|}\\sum\\limits_{i \\in S} y_i $$\n",
    "\n",
    "или среднюю абсолютную ошибку относительно медианы\n",
    "\n",
    "$$I(S) = \\frac{1}{|S|} \\sum\\limits_{i \\in S} |y_i - c| $$ \n",
    "$$ c = median(\\{y_i\\}) \\ i \\in S$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = tree.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='blue')\n",
    "plt.xlabel('mileage')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dec_reg(depth=1, criterion='mse', ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figwidth(20)\n",
    "    \n",
    "    x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(criterion=criterion, max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_hat = tree.predict(x)\n",
    "    ax[1].set_xlabel('mileage')\n",
    "    ax[1].set_ylabel('price')\n",
    "    ax[1].scatter(X_train, y_train, label='actual data')\n",
    "    ax[1].plot(x, y_hat, c='blue', label='decision tree \\nregression')\n",
    "    ax[1].legend(loc=2)\n",
    "    \n",
    "    try:\n",
    "        with open('tree.dot', 'w') as fout:\n",
    "            export_graphviz(tree, out_file=fout, feature_names=['mileage'])\n",
    "        command = [\"dot\", \"-Tpng\", \"tree.dot\", \"-o\", \"tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[0].imshow(plt.imread('tree.png'))\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = interact(plot_dec_reg, depth=IntSlider(min=1, max=5, value=1), criterion=['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование остатков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остаток - residual\n",
    "\n",
    "$resid = y - \\hat{y}$\n",
    "\n",
    "Иногда бывает полезно анализировать попарные графики, связанные с остатками модели. Это может помочь найти выбросы и странное поведение модели, понять перепредсказываем ли мы или наоборот - недопредсказываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_report(X_s, y_s, model, labels, score=mean_absolute_error):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, len(labels)))\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        X = X_s[idx]\n",
    "        y_true = y_s[idx]\n",
    "\n",
    "        y_hat = model.predict(X)\n",
    "\n",
    "        # Scatter\n",
    "        ax[0].scatter(y_hat, y_true, color = colors[idx], alpha=0.3,\n",
    "                      label='%s Score - %2.4f' % (label, score(y_true, y_hat)))\n",
    "\n",
    "        # Resid\n",
    "        resid = y_true - y_hat\n",
    "        ax[1].scatter(resid, y_true, color = colors[idx], alpha=0.3, label=label)\n",
    "\n",
    "        # Distr\n",
    "        ax[2].hist(y_hat, alpha=0.5, label=label, color = colors[idx], normed=True)\n",
    "\n",
    "        # Resid\n",
    "        resid = y_true - y_hat\n",
    "        ax[3].scatter(resid, y_hat, color = colors[idx], alpha=0.3, label=label)\n",
    "\n",
    "    ax[0].legend(loc=4)\n",
    "    ax[0].set_xlabel('$\\hat{y}$')\n",
    "    ax[0].set_ylabel('$y$')\n",
    "\n",
    "    ax[1].legend(loc=2)\n",
    "    ax[1].set_xlabel('$resid$')\n",
    "    ax[1].set_ylabel('$y$')\n",
    "\n",
    "    ax[2].legend(loc=2)\n",
    "    ax[2].set_xlabel('$\\hat{y}$')\n",
    "\n",
    "    ax[3].legend(loc=2)\n",
    "    ax[3].set_xlabel('$resid$')\n",
    "    ax[3].set_ylabel('$\\hat{y}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10, \n",
    "                          weights='uniform', \n",
    "                          metric='manhattan')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report([X_train, X_test], [y_train, y_test], knn,\n",
    "                  ['train', 'test'], score=mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$\\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "То есть необходимо оценить коэффициенты $\\beta_i$.\n",
    "\n",
    "В случае линейной регрессии коэффициенты $\\beta_i$ рассчитываются так, чтобы минимизировать сумму квадратов ошибок по всем наблюдениям:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min $$\n",
    "\n",
    "Как можно найти веса?\n",
    "* Аналитически\n",
    "    * Нестабильное решение\n",
    "    * Долго\n",
    "* Методами оптимизации (градиентный спуск и др)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель\n",
    "$$ a(x) = \\beta_0 + \\beta_1 x_1$$\n",
    "\n",
    "Функция потерь:\n",
    "$$ L(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^n(\\beta_0 + \\beta_1x_1^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "* Предположим мы выбрали какое-то начальное приближение $(\\hat{\\beta_0}, \\hat{\\beta_1})$\n",
    "* Его можно постараться улучшить - надо двигаться в сторону наискорейшего убывания функции (Антиградиента!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stuff import sq_loss_demo, grad_demo, stoch_grad_demo, gradient_descent_upd, stoch_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_loss_demo(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_demo(df_train, iters=105, alpha=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Природа зависимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не всегда переменные зависят друг от друга именно в том виде, в котором они даны. Никто не запрещает зависимость вида\n",
    "$$\\log(y) = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\frac{1}{x_1}$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^2 + \\beta_3 x_1x_2 $$\n",
    "и т.д.\n",
    "\n",
    "Не смотря на то, что могут возникать какие-то нелинейные функции - всё это сводится к **линейной** регрессии (например, о втором пункте, произведите замену $z_1 = \\frac{1}{x_1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные `weights.csv` c информацией о весах мозга и тел различных биологических видов. Вес тела задан в килограммах, вес могза в граммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/weights.csv', sep=';', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.plot(x = 'body_w', y='brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[:2])\n",
    "\n",
    "X = df.body_w.values.reshape(-1,1)\n",
    "y = df.brain_w.values\n",
    "model.fit(X, y)\n",
    "\n",
    "x_range = np.linspace(X.min(), X.max(), 10).reshape(-1, 1)\n",
    "y_hat = model.predict(x_range)\n",
    "plt.plot(x_range, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте возьмем логарифм от обеих переменных и снова нарисуем их на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_body_w'] = np.log(df.body_w)\n",
    "df['log_brain_w'] = np.log(df.brain_w)\n",
    "df.plot(x = 'log_body_w', y='log_brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[2:])\n",
    "    \n",
    "X = df.log_body_w.values.reshape(-1,1)\n",
    "y = df.log_brain_w.values\n",
    "model.fit(X, y)\n",
    "    \n",
    "x_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_hat = model.predict(x_range)\n",
    "plt.plot(x_range, y_hat)\n",
    "\n",
    "y_hat = model.predict(x_range)\n",
    "plt.plot(x_range, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вернемся к начальным координатам\n",
    "\n",
    "x_range = np.exp(x_range)\n",
    "y_hat = np.exp(y_hat)\n",
    "\n",
    "df.plot(x = 'body_w', y='brain_w', kind='scatter')\n",
    "for k, v in df.iterrows():\n",
    "    plt.annotate(k, v[:2])\n",
    "\n",
    "plt.plot(x_range, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ладно, давайте дальше в sklearn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model:\\nprice = %.2f + (%.2f)*mileage' % (model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='mileage', y='price', kind='scatter', s=120)\n",
    "\n",
    "y_hat = model.predict(X_train)\n",
    "plt.plot(X_train, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение\\недообучение, мультиколлинеарность и регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из важнейших характеристик моделей, будь то линейная регрессия, наивные Байес и др. - их **обобщающая способность**.\n",
    "Наша задача не построить \"идеальную\" модель, на имеющихся у нас наблюдениях, которая идеально их будет предсказывать, но и применять эту модель для новых данных.\n",
    "\n",
    "Ниже приводятся примеры 3х моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20%5B8%5D.png>\n",
    "[Andrew's Ng Machine Learning Class - Stanford]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй момент, который важен для линейных моделей - **мультиколлинеарность**. Этот эффект возникает, когда пара предикторов  близка к взаимной линейной зависимости (коэффициент корреляции по модулю близок к 1). Из-за этого:\n",
    "\n",
    "* Не существует аналитического решения\n",
    "* Зависимость $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$ перестаёт быть одназначной (падает интерпретируемость модели)\n",
    "\n",
    "С этим эффектом можно бороться несколькими способами\n",
    "\n",
    "* Последовательно добавлять переменные в модель\n",
    "* Исключать коррелируемые предикторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих случаях может помочь **регуляризация** - добавление штрафного слагаемого за сложность модели в функцию потерь. В случае линейной регрессии было:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\right] + \\lambda\\sum_{j=1}^{m}\\beta_j^2$$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\right] + \\lambda\\sum_{j=1}^{m}|\\beta_j|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в модель ложную переменную - пробег в километрах (в милях он у нас ужее есть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.loc[:, 'kms'] = df_train.mileage*1.6\n",
    "df_test.loc[:, 'kms'] = df_test.mileage*1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.loc[:, ['mileage', 'kms']]\n",
    "y_train = df_train.price.values\n",
    "\n",
    "X_test = df_test.loc[:, ['mileage', 'kms']]\n",
    "y_test = df_test.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=1.)\n",
    "model.fit(X_train, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия и выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Квадратичная ошибка достаточно чувствительна к выбросам. Давайте добавим выбросы.\n",
    "\n",
    "Посмотрим, как поведет себя простая линейная регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values\n",
    "\n",
    "n = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем выброс(-ы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выполним ящейку несколько раз, чтобы добавить выбросы\n",
    "X_train = np.r_[X_train, [[250000+np.random.rand()*10000]]]\n",
    "y_train = np.r_[y_train, 16000+np.random.randn()*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Обучим 2 модели. Первая - на данных без выбросов. Вторая - на всех данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train[:n], y_train[:n])\n",
    "\n",
    "model_ouliers = LinearRegression(fit_intercept=True)\n",
    "model_ouliers.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model.predict(x)\n",
    "y_hat_outliers = model_ouliers.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red')\n",
    "ax.plot(x, y_hat_outliers, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея робастных методов заключается во взвешивании остатков модели таким образом, чтобы большие значения вносили меньший вклад в оценку параметров.\n",
    "\n",
    "Таким образом, вместо минимизации квадрата остатков $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "Будут минимизироваться взвешенные остатки $$ L_w(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}\\rho_i \\cdot (\\hat{y}^{(i)} - y^{(i)})^2,$$\n",
    "где $\\rho_i$ - некоторый вес, которые будет большой для объектов с малой ошибкой и малым для объектов с высокой ошибкой\n",
    "\n",
    "Для того, чтобы попробовать эти методы нужно будет устновить пакет `statsmodels` через `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 4.685\n",
    "support = np.linspace(-3*c, 3*c, 1000)\n",
    "tukey = sm.robust.norms.TukeyBiweight(c=c)\n",
    "plt.plot(support, tukey(support))\n",
    "plt.ylim(.1, -4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный список взвешивающих функций в модуле `statsmodels` можно найти [тут](http://statsmodels.sourceforge.net/stable/examples/notebook/generated/robust_models_1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_robust = sm.RLM(y_train, sm.add_constant(X_train), M=sm.robust.norms.TukeyBiweight())\n",
    "model_robust = model_robust.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_robust.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model_robust.predict(sm.add_constant(x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red')\n",
    "ax.plot(x, y_hat_outliers, c='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавим остальные признаки в простую модель модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним предобработку признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df_input):\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    df_output.drop(['year', 'kms'], axis=1)\n",
    "    \n",
    "    df_output.loc[:, 'transmission'] = \\\n",
    "    df_output.transmission.replace({'Automatic': 0, \n",
    "                                   'Manual': 1})\n",
    "    \n",
    "    df_output.loc[:, 'engine'] = \\\n",
    "    df_output.engine.replace({'4 Cyl': 0, \n",
    "                             '6 Cyl': 1})\n",
    "    \n",
    "    \n",
    "    df_output.loc[:, 'trim'] = df_input.loc[:, 'trim']\\\n",
    "                                      .replace({\n",
    "                                          'ex': 0,\n",
    "                                          'lx': 1,\n",
    "                                          'exl':2\n",
    "                                      })\n",
    "\n",
    "        \n",
    "    return df_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_preprocess = df_train.pipe(preprocess)\n",
    "df_test_preprocess = df_test.pipe(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец-то научимся делать OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Собираем pipeline - сначала one-hot кодирование признаков, потом линейная регрессия\n",
    "\n",
    "model = Pipeline([\n",
    "    ('encoding', OneHotEncoder(categorical_features=[1])),\n",
    "    ('lin_reg', LinearRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "291px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
